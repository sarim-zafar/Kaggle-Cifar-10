{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Doing Array operations if needed\n",
    "import numpy as np\n",
    "\n",
    "#Making the NN\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#Loading images from disk\n",
    "from scipy import misc\n",
    "#Getting the name of all the files\n",
    "import glob\n",
    "#Handling data in df and saving to csv\n",
    "import pandas as pd\n",
    "\n",
    "#One hot encoding of labels\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "#Splitting the data set for train and validation if needed\n",
    "#You can also use validation split\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Handling pickle files\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load target labels\n",
    "train_labels=pd.read_csv('dataset/trainLabels.csv')\n",
    "train_labels.set_index('id', inplace=True)\n",
    "train_labels=train_labels.to_dict(orient='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "#Loading the dataset\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "\n",
    "count=0\n",
    "for image_path in glob.glob(\"dataset/train/*.png\"):\n",
    "    count+=1\n",
    "    if (count%5000==0):\n",
    "        print(count)\n",
    "    x_train.append(misc.imread(image_path))\n",
    "    temp=image_path.split('\\\\')[-1].split('.')[0]\n",
    "    label=train_labels[int(temp)]['label']\n",
    "    y_train.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For Debugging purposes\n",
    "x_train=np.array(x_train)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "lb = LabelBinarizer()\n",
    "y_train=lb.fit_transform(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the labelBinarizer to disk if you dont want to call fit everytime\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(lb, 'label.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This can be a faster way to save/load as compared to loading images everytime\n",
    "file = open('x_train', 'wb')\n",
    "pickle.dump(x_train,file)\n",
    "file.close()\n",
    "file = open('y_train', 'wb')\n",
    "pickle.dump(y_train,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This can be a faster way to save/load as compared to loading images everytime\n",
    "with open('x_train', 'rb') as f:\n",
    "    x_train = pickle.load(f) \n",
    "\n",
    "with open('y_train', 'rb') as f:\n",
    "    y_train = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalizin the images\n",
    "x_train = x_train.astype('float32')\n",
    "x_train/= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train and val split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is used for augmentation of train dataset \n",
    "imdgen = ImageDataGenerator(\n",
    "    featurewise_center = False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center = False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization = False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization = False,  # divide each input by its std\n",
    "    zca_whitening = False,  # apply ZCA whitening\n",
    "    rotation_range = 45,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range = 0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range = 0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip = True,  # randomly flip images\n",
    "    vertical_flip = False,  # randomly flip images\n",
    ")\n",
    "\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "imdgen.fit(x_train)\n",
    "\n",
    "# fit the model on the batches generated by datagen.flow()\n",
    "dgen = imdgen.flow(x_train, y_train, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 30, 30, 64)        1792      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 26, 26, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 11, 11, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 9, 9, 128)         147584    \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1180672   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 1,635,594\n",
      "Trainable params: 1,635,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# For reproducibility\n",
    "np.random.seed(1000)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='elu',kernel_initializer='he_normal', input_shape=(32, 32, 3)))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='elu',kernel_initializer='he_normal'))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='elu',kernel_initializer='he_normal'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='elu',kernel_initializer='he_normal'))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='elu',kernel_initializer='he_normal'))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='elu',kernel_initializer='he_normal'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='elu',kernel_initializer='he_normal'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "checkpoint = ModelCheckpoint(\"CIFAR_10.h5\", monitor='val_loss', verbose=2, save_best_only=True, mode='min',save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adamax',\n",
    "              metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the best weights if you are going to train model from a previous checkpoint\n",
    "\n",
    "#model.load_weights('CIFAR_10.h5')\n",
    "# Train the model with modified images as well\n",
    "#model.fit_generator(dgen,\n",
    "#                    steps_per_epoch=x_train.shape[0]/128,\n",
    "#                    epochs=500,\n",
    "#                    validation_data=(x_val, y_val),\n",
    "#                    workers=2,callbacks=callbacks_list)\n",
    "\n",
    "# Train the model with provided images\n",
    "model.fit(x_train, y_train,batch_size=128,epochs=500,\n",
    "                    validation_data=(x_val, y_val),callbacks=callbacks_list,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n",
      "85000\n",
      "90000\n",
      "95000\n",
      "100000\n",
      "105000\n",
      "110000\n",
      "115000\n",
      "120000\n",
      "125000\n",
      "130000\n",
      "135000\n",
      "140000\n",
      "145000\n",
      "150000\n",
      "155000\n",
      "160000\n",
      "165000\n",
      "170000\n",
      "175000\n",
      "180000\n",
      "185000\n",
      "190000\n",
      "195000\n",
      "200000\n",
      "205000\n",
      "210000\n",
      "215000\n",
      "220000\n",
      "225000\n",
      "230000\n",
      "235000\n",
      "240000\n",
      "245000\n",
      "250000\n",
      "255000\n",
      "260000\n",
      "265000\n",
      "270000\n",
      "275000\n",
      "280000\n",
      "285000\n",
      "290000\n",
      "295000\n",
      "300000\n"
     ]
    }
   ],
   "source": [
    "#making output file\n",
    "#This probably takes a while\n",
    "#I had to do it this way because of limited memory of my laptop 6 gigs :(\n",
    "output=pd.DataFrame(columns=['id','label'])\n",
    "count=0\n",
    "for image_path in glob.glob(\"dataset/test/test/*.png\"):\n",
    "    count+=1\n",
    "    if (count%5000==0):\n",
    "        print(count)\n",
    "    img=misc.imread(image_path).astype('float32')\n",
    "    img/= 255\n",
    "    img=np.array([img])\n",
    "    pred=model.predict(img)\n",
    "    pred=lb.inverse_transform(pred)\n",
    "    index=image_path.split('\\\\')[-1].split('.')[0]\n",
    "    output=output.append({'id':index, 'label':pred[0]}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write the output to a csv\n",
    "output.to_csv('output_custom.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
